{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "IS620 | Text Analysis  \n",
    "Aaron Palumbo  \n",
    "November 20, 2015  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the movie review document classifier  discussed in this chapter, generate a list of  the 30 features that the classifier finds to be most informative. Can you  explain why these particular features are informative? Do you find any of them surprising?\n",
    "\n",
    "We will be pulling from the example in \"Natural Language Processing with Python\" Section 6.1, Document Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies / Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# silly utility to launch a qtconsole if one doesn't exist\n",
    "import psutil\n",
    "\n",
    "def returnPyIDs():\n",
    "    pyids = set()\n",
    "    for pid in psutil.pids():\n",
    "        try:\n",
    "            if \"python\" in psutil.Process(pid).name():\n",
    "                pyids.add(pid)\n",
    "        except:\n",
    "            pass\n",
    "    return pyids\n",
    "\n",
    "def launchConsole():\n",
    "    before_pyids = returnPyIDs()\n",
    "    %qtconsole\n",
    "    after_pyids = returnPyIDs()\n",
    "    newid = after_pyids.difference(before_pyids)\n",
    "    assert len(newid) == 1\n",
    "    return list(newid)[0]\n",
    "\n",
    "try:\n",
    "    qtid\n",
    "except NameError:\n",
    "    qtid = launchConsole()\n",
    "    \n",
    "if qtid not in returnPyIDs():\n",
    "    qtid = launchConsole()\n",
    "    \n",
    "qtid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "            for category in movie_reviews.categories()\n",
    "            for fileid in movie_reviews.fileids(category)]\n",
    "random.seed(1)\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "word_features = all_words.keys()[:2000]\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n",
      "Most Informative Features\n",
      "           contains(ugh) = True              neg : pos    =      9.7 : 1.0\n",
      "          contains(sans) = True              neg : pos    =      8.4 : 1.0\n",
      "    contains(mediocrity) = True              neg : pos    =      7.7 : 1.0\n",
      "     contains(dismissed) = True              pos : neg    =      7.0 : 1.0\n",
      "   contains(bruckheimer) = True              neg : pos    =      6.3 : 1.0\n",
      "         contains(wires) = True              neg : pos    =      6.3 : 1.0\n",
      "        contains(fabric) = True              pos : neg    =      6.3 : 1.0\n",
      "     contains(testament) = True              pos : neg    =      6.2 : 1.0\n",
      "     contains(uplifting) = True              pos : neg    =      5.8 : 1.0\n",
      "        contains(doubts) = True              pos : neg    =      5.8 : 1.0\n",
      "       contains(topping) = True              pos : neg    =      5.7 : 1.0\n",
      "   contains(overwhelmed) = True              pos : neg    =      5.7 : 1.0\n",
      "  contains(effortlessly) = True              pos : neg    =      5.6 : 1.0\n",
      "           contains(hal) = True              neg : pos    =      5.0 : 1.0\n",
      "         contains(pairs) = True              neg : pos    =      5.0 : 1.0\n",
      "        contains(beware) = True              neg : pos    =      5.0 : 1.0\n",
      "          contains(wits) = True              pos : neg    =      5.0 : 1.0\n",
      "          contains(lang) = True              pos : neg    =      5.0 : 1.0\n",
      "         contains(tripe) = True              neg : pos    =      4.6 : 1.0\n",
      "          contains(hugo) = True              pos : neg    =      4.6 : 1.0\n",
      "     contains(overboard) = True              pos : neg    =      4.6 : 1.0\n",
      "    contains(derivative) = True              neg : pos    =      4.4 : 1.0\n",
      "      contains(chopping) = True              neg : pos    =      4.3 : 1.0\n",
      "         contains(gooey) = True              neg : pos    =      4.3 : 1.0\n",
      "         contains(locks) = True              neg : pos    =      4.3 : 1.0\n",
      "      contains(matheson) = True              pos : neg    =      4.3 : 1.0\n",
      "          contains(leia) = True              pos : neg    =      4.3 : 1.0\n",
      "         contains(spins) = True              pos : neg    =      4.3 : 1.0\n",
      "          contains(rico) = True              pos : neg    =      4.3 : 1.0\n",
      "         contains(edges) = True              pos : neg    =      4.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "print nltk.classify.accuracy(classifier, test_set)\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "classifier.show_most_informative_features(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_informative = output.stdout.splitlines()\n",
    "most_informative_neg = [i for i in most_informative if 'neg : pos' in i]\n",
    "most_informative_pos = [i for i in most_informative if 'pos : neg' in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's take a look at the negative indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'           contains(ugh) = True              neg : pos    =      9.7 : 1.0',\n",
       " u'          contains(sans) = True              neg : pos    =      8.4 : 1.0',\n",
       " u'    contains(mediocrity) = True              neg : pos    =      7.7 : 1.0',\n",
       " u'   contains(bruckheimer) = True              neg : pos    =      6.3 : 1.0',\n",
       " u'         contains(wires) = True              neg : pos    =      6.3 : 1.0',\n",
       " u'           contains(hal) = True              neg : pos    =      5.0 : 1.0',\n",
       " u'         contains(pairs) = True              neg : pos    =      5.0 : 1.0',\n",
       " u'        contains(beware) = True              neg : pos    =      5.0 : 1.0',\n",
       " u'         contains(tripe) = True              neg : pos    =      4.6 : 1.0',\n",
       " u'    contains(derivative) = True              neg : pos    =      4.4 : 1.0',\n",
       " u'      contains(chopping) = True              neg : pos    =      4.3 : 1.0',\n",
       " u'         contains(gooey) = True              neg : pos    =      4.3 : 1.0',\n",
       " u'         contains(locks) = True              neg : pos    =      4.3 : 1.0']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_informative_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these make sense and would seem out of place in a positive review are:\n",
    "* ugh\n",
    "* mediocrity\n",
    "* beware\n",
    "* tripe\n",
    "* derivative\n",
    "\n",
    "The words that are less obvious but still fathomable (in my opinion), are:\n",
    "* sans\n",
    "* chopping\n",
    "\n",
    "The words that don't make a lot of sense, and are probably examples of overfitting are:\n",
    "* bruckheimer\n",
    "* wires\n",
    "* hal\n",
    "* pairs\n",
    "* gooey\n",
    "* locks\n",
    "\n",
    "Let's dig a little deeper into this last set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getStats(word_list):\n",
    "    neg_overfit = {}\n",
    "    for w in word_list:\n",
    "        neg_overfit[w] = {}\n",
    "\n",
    "    for w in neg_overfit.keys():\n",
    "        ratings = [rating for (words, rating) in documents if w in words]\n",
    "        neg_overfit[w]['count'] = len(ratings)\n",
    "        neg_overfit[w]['numPos'] = sum([i == 'pos' for i in ratings])\n",
    "        neg_overfit[w]['numNeg'] = len(ratings) - neg_overfit[w]['numPos']\n",
    "    \n",
    "    summary = []\n",
    "    for w in neg_overfit.keys():\n",
    "        obj = neg_overfit[w]\n",
    "        summary.append(\"{:15} => Num Appearances: {:2}, Pos: {:2}, Neg: {:2}\".format \\\n",
    "        (w, obj['count'], obj['numPos'], obj['numNeg']))\n",
    "    return \"\\n\".join(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs           => Num Appearances:  9, Pos:  2, Neg:  7\n",
      "wires           => Num Appearances: 10, Pos:  1, Neg:  9\n",
      "hal             => Num Appearances: 10, Pos:  2, Neg:  8\n",
      "bruckheimer     => Num Appearances: 10, Pos:  1, Neg:  9\n",
      "gooey           => Num Appearances:  8, Pos:  2, Neg:  6\n",
      "locks           => Num Appearances:  7, Pos:  1, Neg:  6\n"
     ]
    }
   ],
   "source": [
    "print getStats(\"bruckheimer wires hal pairs gooey locks\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These words do not appear very many times in the dataset and should probably not be used. In the beginning, when we decided to take the 2000 most frequently appearing words we probably overreached.\n",
    "\n",
    "For a comparison, let's take a look at the words we flagged as making sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tripe           => Num Appearances: 13, Pos:  2, Neg: 11\n",
      "mediocrity      => Num Appearances: 12, Pos:  1, Neg: 11\n",
      "beware          => Num Appearances: 10, Pos:  2, Neg:  8\n",
      "ugh             => Num Appearances: 16, Pos:  2, Neg: 14\n",
      "derivative      => Num Appearances: 22, Pos:  5, Neg: 17\n"
     ]
    }
   ],
   "source": [
    "print getStats(\"ugh mediocrity beware tripe derivative\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better, but not as much difference as you might expect. Maybe what we're really seeing is just a lack of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the positive indicators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'     contains(dismissed) = True              pos : neg    =      7.0 : 1.0',\n",
       " u'        contains(fabric) = True              pos : neg    =      6.3 : 1.0',\n",
       " u'     contains(testament) = True              pos : neg    =      6.2 : 1.0',\n",
       " u'     contains(uplifting) = True              pos : neg    =      5.8 : 1.0',\n",
       " u'        contains(doubts) = True              pos : neg    =      5.8 : 1.0',\n",
       " u'       contains(topping) = True              pos : neg    =      5.7 : 1.0',\n",
       " u'   contains(overwhelmed) = True              pos : neg    =      5.7 : 1.0',\n",
       " u'  contains(effortlessly) = True              pos : neg    =      5.6 : 1.0',\n",
       " u'          contains(wits) = True              pos : neg    =      5.0 : 1.0',\n",
       " u'          contains(lang) = True              pos : neg    =      5.0 : 1.0',\n",
       " u'          contains(hugo) = True              pos : neg    =      4.6 : 1.0',\n",
       " u'     contains(overboard) = True              pos : neg    =      4.6 : 1.0',\n",
       " u'      contains(matheson) = True              pos : neg    =      4.3 : 1.0',\n",
       " u'          contains(leia) = True              pos : neg    =      4.3 : 1.0',\n",
       " u'         contains(spins) = True              pos : neg    =      4.3 : 1.0',\n",
       " u'          contains(rico) = True              pos : neg    =      4.3 : 1.0',\n",
       " u'         contains(edges) = True              pos : neg    =      4.3 : 1.0']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_informative_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lang            => Num Appearances:  9, Pos:  8, Neg:  1\n",
      "topping         => Num Appearances:  9, Pos:  8, Neg:  1\n",
      "effortlessly    => Num Appearances: 22, Pos: 19, Neg:  3\n",
      "fabric          => Num Appearances: 10, Pos:  9, Neg:  1\n",
      "wits            => Num Appearances:  9, Pos:  8, Neg:  1\n",
      "hugo            => Num Appearances: 13, Pos: 11, Neg:  2\n",
      "matheson        => Num Appearances:  7, Pos:  6, Neg:  1\n",
      "dismissed       => Num Appearances: 11, Pos: 10, Neg:  1\n",
      "rico            => Num Appearances: 10, Pos:  8, Neg:  2\n",
      "edges           => Num Appearances:  8, Pos:  6, Neg:  2\n",
      "doubts          => Num Appearances: 16, Pos: 14, Neg:  2\n",
      "testament       => Num Appearances: 21, Pos: 17, Neg:  4\n",
      "leia            => Num Appearances:  8, Pos:  6, Neg:  2\n",
      "overboard       => Num Appearances: 15, Pos: 11, Neg:  4\n",
      "overwhelmed     => Num Appearances: 10, Pos:  9, Neg:  1\n",
      "spins           => Num Appearances:  7, Pos:  6, Neg:  1\n",
      "uplifting       => Num Appearances: 24, Pos: 21, Neg:  3\n"
     ]
    }
   ],
   "source": [
    "pos_words = [i.split(\"(\")[1][:-1] for i in \n",
    "             [j for j in \"\".join(most_informative_pos).split() if 'contains' in j]]\n",
    "print getStats(pos_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, some of the words in this list make sense (effortlessly, overwhelmed), but some of the do not (leia, spins). There is some indication that the words that appear more often are stronger predictors, but what we really need is more data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
